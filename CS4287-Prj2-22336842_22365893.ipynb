{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a00b930c",
   "metadata": {},
   "source": [
    "Add 2-3 sentences about catastrophic forgetting and how experience replay addresses it\n",
    "Expand evaluation section to mention why accuracy/precision/recall don't apply to RL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b75af1",
   "metadata": {},
   "source": [
    "**Team Members:** Michael Cronin (22336842), Darren Nugent (22365893)  \n",
    "**Code Executes to End:** Yes  \n",
    "**Third-Party Code:** Original implementation based on DQN architecture from Mnih et al. (2015) and Double DQN from van Hasselt et al. (2016). No external DQN libraries used.\n",
    "We chose the Atari game Space Invaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d57993",
   "metadata": {},
   "source": [
    "# CS4287 Assignment 2 Option 2: Deep Q-Learning for Space Invaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff44dbd",
   "metadata": {},
   "source": [
    "## 1. Why Reinforcement Learning for Space Invaders?\n",
    "\n",
    "Space Invaders requires an agent to make sequential decisions in a dynamic environment where actions have delayed consequences. Reinforcement Learning is the appropriate paradigm for several reasons:\n",
    "\n",
    "**Sequential Decision Making:** The game requires planning over time - positioning to avoid alien fire while targeting specific aliens for maximum points. RL naturally handles this through its framework of states, actions, and cumulative rewards.\n",
    "\n",
    "**Trial and Error Learning:** Like a human player, an RL agent learns through interaction with the game environment. It discovers that shooting aliens yields rewards while getting hit ends the episode, without requiring explicit programming of these rules.\n",
    "\n",
    "**High-Dimensional Visual Input:** The game state is represented as 210×160 RGB images. Deep Q-Networks combine deep learning's ability to process high-dimensional inputs with RL's sequential decision-making framework.\n",
    "\n",
    "**Delayed Rewards:** Strategic decisions like preserving shields or targeting the mystery ship require understanding delayed consequences - a core strength of RL's reward accumulation over time.\n",
    "\n",
    "**Dynamic Difficulty:** As aliens are destroyed, the remaining ones speed up. RL agents continuously update their policy based on the current state, naturally adapting to this increasing difficulty."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a209c0",
   "metadata": {},
   "source": [
    "## 2. The Gymnasium Environment: Space Invaders\n",
    "\n",
    "### Environment Specification\n",
    "- **Environment ID:** ALE/SpaceInvaders-v5\n",
    "- **Observation Space:** Box(0, 255, (210, 160, 3), uint8) - RGB frames\n",
    "- **Action Space:** Discrete(6)\n",
    "- **Lives:** 3 per episode\n",
    "\n",
    "### Action Space\n",
    "Space Invaders provides 6 discrete actions:\n",
    "\n",
    "| Action ID | Action Name | Description |\n",
    "|-----------|-------------|-------------|\n",
    "| 0 | NOOP | No operation |\n",
    "| 1 | FIRE | Shoot while stationary |\n",
    "| 2 | RIGHT | Move right |\n",
    "| 3 | LEFT | Move left |\n",
    "| 4 | RIGHTFIRE | Move right and shoot |\n",
    "| 5 | LEFTFIRE | Move left and shoot |\n",
    "\n",
    "### Reward Structure\n",
    "Points are awarded for destroying aliens, with higher rows worth more:\n",
    "\n",
    "| Target | Points |\n",
    "|--------|--------|\n",
    "| Bottom row aliens | 10 |\n",
    "| Second row | 20 |\n",
    "| Third row | 30 |\n",
    "| Top row aliens | 40 |\n",
    "| Mystery ship (UFO) | 50-200 (random) |\n",
    "\n",
    "### Role in Training\n",
    "The environment provides 210×160×3 RGB observations at each timestep. These raw frames are preprocessed to 84×84 grayscale images and stacked in groups of 4 to provide temporal information about motion and velocity. The reward signal guides the agent's policy optimization through the Q-learning algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18354e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ale_py\n",
    "%pip install gymnasium[atari]\n",
    "%pip install matplotlib\n",
    "%pip install imageio\n",
    "%pip install imageio-ffmpeg\n",
    "%pip install opencv-python\n",
    "%pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d95e695",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ale_py\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import os\n",
    "import imageio\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa6c038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the envirnment\n",
    "gym.register_envs(ale_py)\n",
    "env = gym.make('ALE/SpaceInvaders-v5', render_mode='rgb_array', frameskip=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50931b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment analysis\n",
    "obs, info = env.reset()\n",
    "\n",
    "print(f\"OBSERVATION SPACE\")\n",
    "print(f\"Shape: {obs.shape}\")\n",
    "print(f\"Data type: {obs.dtype}\")\n",
    "print(f\"Min value: {obs.min()}, Max value: {obs.max()}\")\n",
    "print(f\"Total pixels: {np.prod(obs.shape):,}\")\n",
    "\n",
    "print(f\"\\nACTION SPACE\")\n",
    "print(f\"Type: {env.action_space}\")\n",
    "print(f\"Number of actions: {env.action_space.n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1688858e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See what each action does\n",
    "action_meanings = env.unwrapped.get_action_meanings()\n",
    "for i, action in enumerate(action_meanings):\n",
    "    print(f\"Action {i}: {action}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a54649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visulaise initial state\n",
    "plt.figure(figsize=(8, 10))\n",
    "plt.imshow(obs)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25eec913",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a8fd5a",
   "metadata": {},
   "source": [
    "## 3. Implementation\n",
    "\n",
    "### 3.1 Data Preprocessing\n",
    "\n",
    "Raw Atari frames are 210×160×3 RGB images, which is computationally expensive and contains unnecessary information. Our preprocessing pipeline applies the following transformations:\n",
    "\n",
    "1. **Frame skip (k=3):** Unlike most Atari games which use k=4, Space Invaders requires k=3 because the laser projectiles blink at a frequency that makes them invisible with k=4 (this is the only hyperparameter difference noted in the original DQN paper)\n",
    "2. **Cropping:** Remove score display and ground (rows 20-195), reducing visual noise\n",
    "3. **Grayscale conversion:** Reduce from 3 channels to 1 (color not needed for gameplay)\n",
    "4. **Resize:** Downsample to 84×84 for computational efficiency\n",
    "5. **Normalization:** Scale pixel values from [0, 255] to [0, 1]\n",
    "6. **Frame stacking:** Stack 4 consecutive frames to capture motion and velocity information\n",
    "\n",
    "This reduces the input from 100,800 values per frame to 7,056 values per frame, while the 4-frame stack provides temporal context that allows the network to perceive movement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316d4d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpaceInvadersPreprocessor(gym.Wrapper):\n",
    "    def __init__(self, env, frame_stack=4, im_size=84):\n",
    "        super().__init__(env)\n",
    "        self.frame_stack = frame_stack\n",
    "        self.im_size = im_size\n",
    "        self.frames = deque(maxlen=frame_stack)\n",
    "        \n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=0.0,\n",
    "            high=1.0,\n",
    "            shape=(frame_stack, im_size, im_size),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "    \n",
    "    def preprocess_frame(self, frame):\n",
    "        # Crop, grayscale, resize, normalize\n",
    "        cropped = frame[20:195, :]\n",
    "        gray = cv2.cvtColor(cropped, cv2.COLOR_RGB2GRAY)\n",
    "        resized = cv2.resize(gray, (self.im_size, self.im_size), \n",
    "                            interpolation=cv2.INTER_AREA)\n",
    "        normalised = resized.astype(np.float32) / 255.0\n",
    "        return normalised\n",
    "    \n",
    "    def reset(self, **kwargs):\n",
    "        obs, info = self.env.reset(**kwargs)\n",
    "        processed = self.preprocess_frame(obs)\n",
    "        \n",
    "        for _ in range(self.frame_stack):\n",
    "            self.frames.append(processed)\n",
    "        \n",
    "        return np.array(self.frames, dtype=np.float32), info\n",
    "    \n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "        processed = self.preprocess_frame(obs)\n",
    "        self.frames.append(processed)\n",
    "        \n",
    "        return np.array(self.frames, dtype=np.float32), reward, terminated, truncated, info\n",
    "\n",
    "\n",
    "# Quick test\n",
    "env = gym.make('ALE/SpaceInvaders-v5', frameskip=3)\n",
    "env = SpaceInvadersPreprocessor(env)\n",
    "state, _ = env.reset()\n",
    "print(f\"State shape: {state.shape}, range: [{state.min():.2f}, {state.max():.2f}]\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf5bed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environments\n",
    "env_orig = gym.make('ALE/SpaceInvaders-v5', render_mode='rgb_array', frameskip=3)\n",
    "env_wrap = SpaceInvadersPreprocessor(gym.make('ALE/SpaceInvaders-v5', render_mode='rgb_array', frameskip=3))\n",
    "\n",
    "# Get frames\n",
    "orig_frame, _ = env_orig.reset(seed=42)\n",
    "preprocessed, _ = env_wrap.reset(seed=42)\n",
    "\n",
    "# Take a few steps to see actual gameplay\n",
    "for _ in range(15):\n",
    "    orig_frame, _, _, _, _ = env_orig.step(1)  # FIRE action\n",
    "    preprocessed, _, _, _, _ = env_wrap.step(1)\n",
    "\n",
    "# Plot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "ax1.imshow(orig_frame)\n",
    "ax1.set_title(f'Original: {orig_frame.shape}')\n",
    "ax1.axis('off')\n",
    "\n",
    "ax2.imshow(preprocessed[0], cmap='gray')  # Show first frame from stack\n",
    "ax2.set_title(f'Preprocessed: {preprocessed.shape}')\n",
    "ax2.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Original: {orig_frame.shape}, dtype={orig_frame.dtype}, range=[{orig_frame.min()}, {orig_frame.max()}]\")\n",
    "print(f\"Preprocessed: {preprocessed.shape}, dtype={preprocessed.dtype}, range=[{preprocessed.min():.2f}, {preprocessed.max():.2f}]\")\n",
    "\n",
    "env_orig.close()\n",
    "env_wrap.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b65ba8",
   "metadata": {},
   "source": [
    "### 3.2 Network Architecture\n",
    "\n",
    "The DQN follows the architecture from Mnih et al. (2015):\n",
    "\n",
    "**Convolutional Layers:**\n",
    "- Conv1: 32 filters, 8×8 kernel, stride 4 → extracts low-level features (edges, shapes)\n",
    "- Conv2: 64 filters, 4×4 kernel, stride 2 → mid-level features (alien patterns, shields)\n",
    "- Conv3: 64 filters, 3×3 kernel, stride 1 → high-level features (spatial relationships)\n",
    "\n",
    "**Fully Connected Layers:**\n",
    "- FC1: 3136 → 512 neurons with ReLU activation\n",
    "- FC2: 512 → 6 neurons (Q-values for each action)\n",
    "\n",
    "The convolutional layers extract spatial features from the stacked frames, while the fully connected layers map these features to action values. ReLU activations introduce non-linearity, allowing the network to learn complex policies.\n",
    "\n",
    "**Hyperparameter Summary:**\n",
    "\n",
    "| Parameter | Value | Notes |\n",
    "|-----------|-------|-------|\n",
    "| Replay buffer size | 100,000 | Stores recent transitions |\n",
    "| Batch size | 32 | Samples per training step |\n",
    "| Discount factor (γ) | 0.99 | High value for long-term planning |\n",
    "| Learning rate | 1e-4 | Adam optimizer |\n",
    "| Initial epsilon | 1.0 | Start with full exploration |\n",
    "| Final epsilon | 0.01 | End with 1% exploration |\n",
    "| Epsilon decay | 0.9995 | Multiplicative decay per step |\n",
    "| Target network update | 1000 steps | Hard update frequency |\n",
    "| Frame skip | 3 | Space Invaders specific |\n",
    "| Frame stack | 4 | Temporal context |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7b45a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, n_actions=6):\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        # Conv layers\n",
    "        self.conv1 = nn.Conv2d(4, 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        \n",
    "        # FC layers\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 512)\n",
    "        self.fc2 = nn.Linear(512, n_actions)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.view(x.size(0), -1)  # flatten\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "\n",
    "# Test it\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = DQN(n_actions=6).to(device)\n",
    "\n",
    "# Test with dummy input (batch of 1, 4 frames, 84x84)\n",
    "test_input = torch.randn(1, 4, 84, 84).to(device)\n",
    "output = model(test_input)\n",
    "print(f\"Input: {test_input.shape}, Output: {output.shape}\")\n",
    "print(f\"Running on: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3712559",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=100000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        \n",
    "        return (\n",
    "            torch.FloatTensor(np.array(states)),\n",
    "            torch.LongTensor(actions),\n",
    "            torch.FloatTensor(rewards),\n",
    "            torch.FloatTensor(np.array(next_states)),\n",
    "            torch.FloatTensor(dones)\n",
    "        )\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "# Test\n",
    "buffer = ReplayBuffer(capacity=1000)\n",
    "buffer.push(np.random.rand(4, 84, 84), 2, 10, np.random.rand(4, 84, 84), False)\n",
    "print(f\"Buffer size: {len(buffer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9bb51c",
   "metadata": {},
   "source": [
    "### 3.3 Q-Learning Update Mechanism\n",
    " \n",
    " The core of DQN is the Q-learning update, implemented in `train_step()`. Here's exactly how it works:\n",
    " \n",
    " **The Bellman Equation:**\n",
    " The optimal Q-value for a state-action pair satisfies:\n",
    " ```\n",
    " Q*(s, a) = r + γ * max_a' Q*(s', a')\n",
    " ```\n",
    " Where `r` is the immediate reward, `γ` (gamma) is the discount factor, and `s'` is the next state.\n",
    " \n",
    " **Step-by-step through train_step():**\n",
    " \n",
    " 1. **Sample batch from replay buffer:** We randomly sample 32 transitions (s, a, r, s', done) to break correlation between consecutive experiences.\n",
    " \n",
    " 2. **Compute current Q-values:** `current_q = online_net(states).gather(1, actions)` - The network outputs Q-values for ALL actions, then `.gather()` selects only the Q-values for actions we actually took.\n",
    " \n",
    " 3. **Compute target Q-values:**\n",
    "    - **Standard DQN:** `next_q = target_net(next_states).max(1)[0]` - Target network estimates max Q-value at next state\n",
    "    - **Double DQN:** Online network SELECTS best action, target network EVALUATES it - this reduces overestimation bias\n",
    "    - `target_q = rewards + gamma * next_q * (1 - dones)` - If done=1 (terminal), future reward is zero\n",
    " \n",
    " 4. **Compute loss:** `F.smooth_l1_loss(current_q, target_q)` - Huber loss is more robust to outliers than MSE, preventing large gradient updates from unusual experiences.\n",
    " \n",
    " 5. **Backpropagate and update:** Standard gradient descent with gradient clipping (max_norm=10) for stability.\n",
    " \n",
    " 6. **Update target network:** Every 1000 steps, copy online network weights to target network. This provides stable targets during training.\n",
    " \n",
    "**Why Two Networks?**\n",
    "Without a separate target network, we'd be updating towards a moving target (since the same network provides both predictions and targets). The target network provides stable Q-value estimates for several thousand steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa208ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, n_actions=6, double_dqn=False):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.n_actions = n_actions\n",
    "        self.double_dqn = double_dqn  # Toggle for Double DQN\n",
    "        \n",
    "        # Networks\n",
    "        self.online_net = DQN(n_actions).to(self.device)\n",
    "        self.target_net = DQN(n_actions).to(self.device)\n",
    "        self.target_net.load_state_dict(self.online_net.state_dict())\n",
    "        \n",
    "        # Training\n",
    "        self.optimizer = optim.Adam(self.online_net.parameters(), lr=1e-4)\n",
    "        self.replay_buffer = ReplayBuffer(capacity=100000)\n",
    "        \n",
    "        # Hyperparameters\n",
    "        self.gamma = 0.99\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        # self.epsilon_decay = 0.9995\n",
    "        self.epsilon_decay_frames = 1_000_000 #(Nature paper standard)\n",
    "        self.batch_size = 32\n",
    "        self.target_update_freq = 1000\n",
    "        self.steps = 0\n",
    "    \n",
    "    def choose_action(self, state, training=True):\n",
    "        # Epsilon-greedy\n",
    "        if training and random.random() < self.epsilon:\n",
    "            return random.randrange(self.n_actions)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "            q_values = self.online_net(state_tensor)\n",
    "            return q_values.argmax().item()\n",
    "    \n",
    "    def train_step(self):\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return None\n",
    "        \n",
    "        # Sample batch\n",
    "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(self.batch_size)\n",
    "        states = states.to(self.device)\n",
    "        actions = actions.to(self.device)\n",
    "        rewards = rewards.to(self.device)\n",
    "        next_states = next_states.to(self.device)\n",
    "        dones = dones.to(self.device)\n",
    "        \n",
    "        # Current Q values\n",
    "        current_q = self.online_net(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        # Target Q values\n",
    "        with torch.no_grad():\n",
    "            if self.double_dqn:\n",
    "                # Double DQN: online network selects, target network evaluates\n",
    "                next_actions = self.online_net(next_states).argmax(1, keepdim=True)\n",
    "                next_q = self.target_net(next_states).gather(1, next_actions).squeeze(1)\n",
    "            else:\n",
    "                # Standard DQN: target network does both\n",
    "                next_q = self.target_net(next_states).max(1)[0]\n",
    "            \n",
    "            target_q = rewards + self.gamma * next_q * (1 - dones)\n",
    "        \n",
    "        # Compute loss and update\n",
    "        loss = F.smooth_l1_loss(current_q, target_q)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.online_net.parameters(), max_norm=10)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Update target network\n",
    "        self.steps += 1\n",
    "        if self.steps % self.target_update_freq == 0:\n",
    "            self.target_net.load_state_dict(self.online_net.state_dict())\n",
    "        \n",
    "        # Decay epsilon\n",
    "        # self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "        # Epsilon decay moved to train() function - done once per episode, not per step\n",
    "        \n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854f03f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(episodes=1000, log_interval=10, double_dqn=False, save_dir='outputs'):\n",
    "    import os\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    env = gym.make('ALE/SpaceInvaders-v5', frameskip=3)\n",
    "    env = SpaceInvadersPreprocessor(env)\n",
    "    agent = DQNAgent(n_actions=6, double_dqn=double_dqn)\n",
    "    \n",
    "    mode = 'double_dqn' if double_dqn else 'standard_dqn'\n",
    "    print(f\"Training {mode.replace('_', ' ').title()} on {agent.device}\")\n",
    "    \n",
    "    history = {\n",
    "        'episode_rewards': [],\n",
    "        'losses': [],\n",
    "        'epsilons': [],\n",
    "        'avg_q_values': [],\n",
    "        'episode_lengths': [],\n",
    "        'total_frames': []\n",
    "    }\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        # Epsilon decay based on frames (Nature paper: linear decay over 1M frames)\n",
    "        if agent.steps < agent.epsilon_decay_frames:\n",
    "            agent.epsilon = 1.0 - 0.99 * (agent.steps / agent.epsilon_decay_frames)\n",
    "        else:\n",
    "            agent.epsilon = agent.epsilon_min\n",
    "        \n",
    "        state, _ = env.reset()\n",
    "        episode_reward = 0\n",
    "        episode_loss = []\n",
    "        episode_q_values = []\n",
    "        steps = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            action = agent.choose_action(state)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            agent.replay_buffer.push(state, action, reward, next_state, float(done))\n",
    "            \n",
    "            loss = agent.train_step()\n",
    "            if loss is not None:\n",
    "                episode_loss.append(loss)\n",
    "                \n",
    "                # Track Q-values periodically\n",
    "                if steps % 100 == 0:\n",
    "                    with torch.no_grad():\n",
    "                        state_t = torch.FloatTensor(state).unsqueeze(0).to(agent.device)\n",
    "                        q_vals = agent.online_net(state_t)\n",
    "                        episode_q_values.append(q_vals.max().item())\n",
    "            \n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            steps += 1\n",
    "        \n",
    "        # Record history\n",
    "        history['episode_rewards'].append(episode_reward)\n",
    "        history['losses'].append(np.mean(episode_loss) if episode_loss else 0)\n",
    "        history['epsilons'].append(agent.epsilon)\n",
    "        history['avg_q_values'].append(np.mean(episode_q_values) if episode_q_values else 0)\n",
    "        history['episode_lengths'].append(steps)\n",
    "        history['total_frames'].append(agent.steps)\n",
    "        \n",
    "        if episode % log_interval == 0:\n",
    "            avg_reward = np.mean(history['episode_rewards'][-100:])\n",
    "            avg_loss = np.mean(history['losses'][-100:]) if history['losses'] else 0\n",
    "            print(f\"Episode {episode:4d} | Reward: {episode_reward:6.0f} | \"\n",
    "                  f\"Avg100: {avg_reward:7.1f} | Loss: {avg_loss:.4f} | \"\n",
    "                  f\"Epsilon: {agent.epsilon:.3f}\")\n",
    "        \n",
    "        # Save checkpoints\n",
    "        if episode > 0 and episode % 100 == 0:\n",
    "            checkpoint_path = f'{save_dir}/dqn_{mode}_ep{episode}.pth'\n",
    "            torch.save({\n",
    "                'episode': episode,\n",
    "                'model_state_dict': agent.online_net.state_dict(),\n",
    "                'optimizer_state_dict': agent.optimizer.state_dict(),\n",
    "                'epsilon': agent.epsilon,\n",
    "                'history': history\n",
    "            }, checkpoint_path)\n",
    "            print(f\"  Saved checkpoint: {checkpoint_path}\")\n",
    "    \n",
    "    # Save final model\n",
    "    final_path = f'{save_dir}/dqn_{mode}_final.pth'\n",
    "    torch.save({\n",
    "        'model_state_dict': agent.online_net.state_dict(),\n",
    "        'history': history\n",
    "    }, final_path)\n",
    "    \n",
    "    env.close()\n",
    "    return agent, history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30667fd",
   "metadata": {},
   "source": [
    " ## 4. Training Plots\n",
    " \n",
    " The following visualizations track the agent's learning progress:\n",
    " - **Episode Rewards:** Raw and smoothed rewards over training\n",
    " - **Loss Curve:** TD error magnitude over time\n",
    " - **Epsilon Decay:** Exploration rate schedule\n",
    " - **Q-Value Estimates:** Average predicted Q-values (indicator of value learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468761e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_curves(history, title_prefix='DQN', save_path=None):    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # Plot 1: Episode Rewards with multiple moving averages\n",
    "    ax1 = axes[0, 0]\n",
    "    rewards = history['episode_rewards']\n",
    "    ax1.plot(rewards, alpha=0.3, color='blue', label='Raw')\n",
    "    \n",
    "    for window, color in [(50, 'orange'), (100, 'red')]:\n",
    "        if len(rewards) >= window:\n",
    "            ma = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "            ax1.plot(range(window-1, len(rewards)), ma, \n",
    "                    label=f'{window}-Episode MA', color=color, linewidth=2)\n",
    "    \n",
    "    ax1.set_xlabel('Episode')\n",
    "    ax1.set_ylabel('Total Reward')\n",
    "    ax1.set_title(f'{title_prefix}: Episode Rewards')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Training Loss\n",
    "    ax2 = axes[0, 1]\n",
    "    losses = history['losses']\n",
    "    ax2.plot(losses, alpha=0.3, color='blue', label='Raw')\n",
    "    \n",
    "    if len(losses) >= 50:\n",
    "        loss_ma = np.convolve(losses, np.ones(50)/50, mode='valid')\n",
    "        ax2.plot(range(49, len(losses)), loss_ma, \n",
    "                color='red', linewidth=2, label='50-Episode MA')\n",
    "    \n",
    "    ax2.set_xlabel('Episode')\n",
    "    ax2.set_ylabel('Average Loss')\n",
    "    ax2.set_title(f'{title_prefix}: Training Loss')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Epsilon Decay\n",
    "    ax3 = axes[1, 0]\n",
    "    ax3.plot(history['epsilons'], color='green', linewidth=2)\n",
    "    ax3.set_xlabel('Episode')\n",
    "    ax3.set_ylabel('Epsilon')\n",
    "    ax3.set_title(f'{title_prefix}: Exploration Rate (Epsilon)')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    ax3.set_ylim(0, 1.05)\n",
    "    \n",
    "    # Plot 4: Average Q-Values\n",
    "    ax4 = axes[1, 1]\n",
    "    q_values = history['avg_q_values']\n",
    "    ax4.plot(q_values, alpha=0.3, color='purple', label='Raw')\n",
    "    \n",
    "    if len(q_values) >= 50:\n",
    "        q_ma = np.convolve(q_values, np.ones(50)/50, mode='valid')\n",
    "        ax4.plot(range(49, len(q_values)), q_ma, \n",
    "                color='darkviolet', linewidth=2, label='50-Episode MA')\n",
    "    \n",
    "    ax4.set_xlabel('Episode')\n",
    "    ax4.set_ylabel('Average Q-Value')\n",
    "    ax4.set_title(f'{title_prefix}: Q-Value Estimates')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        print(f\"Saved plot: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f48e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot reward variance over training\n",
    "def plot_variance(history, window=50, save_path=None):\n",
    "    \"\"\"Plot variance of rewards over training.\"\"\"\n",
    "    rewards = history['episode_rewards']\n",
    "    \n",
    "    variances = []\n",
    "    for i in range(window, len(rewards)):\n",
    "        variances.append(np.var(rewards[i-window:i]))\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(window, len(rewards)), variances, color='green', alpha=0.7)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Variance')\n",
    "    plt.title(f'Reward Variance (window={window})')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144e5b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_comparison(history_standard, history_double, save_path=None):\n",
    "    \"\"\"Compare Standard DQN vs Double DQN performance.\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Reward comparison\n",
    "    ax1 = axes[0]\n",
    "    for history, label, color in [(history_standard, 'Standard DQN', 'blue'),\n",
    "                                   (history_double, 'Double DQN', 'red')]:\n",
    "        rewards = history['episode_rewards']\n",
    "        if len(rewards) >= 100:\n",
    "            ma = np.convolve(rewards, np.ones(100)/100, mode='valid')\n",
    "            ax1.plot(range(99, len(rewards)), ma, label=label, color=color, linewidth=2)\n",
    "    \n",
    "    ax1.set_xlabel('Episode')\n",
    "    ax1.set_ylabel('Average Reward (100-ep)')\n",
    "    ax1.set_title('Reward Comparison: Standard vs Double DQN')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Q-value comparison\n",
    "    ax2 = axes[1]\n",
    "    for history, label, color in [(history_standard, 'Standard DQN', 'blue'),\n",
    "                                   (history_double, 'Double DQN', 'red')]:\n",
    "        q_vals = history['avg_q_values']\n",
    "        if len(q_vals) >= 50:\n",
    "            ma = np.convolve(q_vals, np.ones(50)/50, mode='valid')\n",
    "            ax2.plot(range(49, len(q_vals)), ma, label=label, color=color, linewidth=2)\n",
    "    \n",
    "    ax2.set_xlabel('Episode')\n",
    "    ax2.set_ylabel('Average Q-Value')\n",
    "    ax2.set_title('Q-Value Estimates: Standard vs Double DQN')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c07f326",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rewards_vs_frames(history, title_prefix='DQN', save_path=None):\n",
    "    \"\"\"Plot rewards over total frames (not episodes).\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    # Get cumulative frame count at each episode end\n",
    "    frames = history['total_frames']\n",
    "    rewards = history['episode_rewards']\n",
    "    \n",
    "    # Plot raw\n",
    "    ax.scatter(frames, rewards, alpha=0.3, s=10, color='blue', label='Raw')\n",
    "    \n",
    "    # Moving average\n",
    "    if len(rewards) >= 100:\n",
    "        ma = np.convolve(rewards, np.ones(100)/100, mode='valid')\n",
    "        ax.plot(frames[99:], ma, color='red', linewidth=2, label='100-Episode MA')\n",
    "    \n",
    "    ax.set_xlabel('Frames')\n",
    "    ax.set_ylabel('Episode Reward')\n",
    "    ax.set_title(f'{title_prefix}: Rewards vs Frames')\n",
    "    ax.axvline(x=1e6, color='gray', linestyle='--', alpha=0.5, label='1M frames')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfafa49",
   "metadata": {},
   "source": [
    " ## 5. Video Recording\n",
    " \n",
    " We record gameplay videos to visually demonstrate learning progress:\n",
    " - Pre-training (random policy)\n",
    " - Post-training (learned policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c42972",
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_video(agent, filename, episodes=1, max_steps=2000, fps=30):\n",
    "    \"\"\"Record gameplay video of the agent.\"\"\"\n",
    "    \n",
    "    env = gym.make('ALE/SpaceInvaders-v5', render_mode='rgb_array', frameskip=3)\n",
    "    env = SpaceInvadersPreprocessor(env)\n",
    "    \n",
    "    frames = []\n",
    "    total_reward = 0\n",
    "    \n",
    "    for ep in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        steps = 0\n",
    "        \n",
    "        while not done and steps < max_steps:\n",
    "            # Get raw frame for video\n",
    "            raw_frame = env.env.render()\n",
    "            frames.append(raw_frame)\n",
    "            \n",
    "            # Agent selects action (no exploration for evaluation)\n",
    "            action = agent.choose_action(state, training=False)\n",
    "            state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "    \n",
    "    env.close()\n",
    "    \n",
    "    # Save as video using imageio\n",
    "    import imageio\n",
    "    imageio.mimsave(filename, frames, fps=fps)\n",
    "    print(f\"Saved video: {filename} ({len(frames)} frames, reward: {total_reward})\")\n",
    "    \n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6bf04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_random_agent(filename, episodes=1, max_steps=2000, fps=30):\n",
    "    \"\"\"Record a random agent for baseline comparison.\"\"\"\n",
    "    \n",
    "    env = gym.make('ALE/SpaceInvaders-v5', render_mode='rgb_array', frameskip=3)\n",
    "    \n",
    "    frames = []\n",
    "    total_reward = 0\n",
    "    \n",
    "    for ep in range(episodes):\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "        steps = 0\n",
    "        \n",
    "        while not done and steps < max_steps:\n",
    "            frames.append(env.render())\n",
    "            action = env.action_space.sample()\n",
    "            obs, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "    \n",
    "    env.close()\n",
    "    \n",
    "    import imageio\n",
    "    imageio.mimsave(filename, frames, fps=fps)\n",
    "    print(f\"Saved random agent video: {filename} (reward: {total_reward})\")\n",
    "    \n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69acea0",
   "metadata": {},
   "source": [
    " ## 6. Evaluation of Results\n",
    " \n",
    " ### How to Evaluate RL Performance\n",
    " \n",
    " Unlike supervised learning where we use accuracy, precision, and recall, RL agents are evaluated through different metrics:\n",
    " \n",
    " **Primary Metrics:**\n",
    " - **Average Episode Reward:** The main performance indicator - higher is better\n",
    " - **Maximum Score Achieved:** Shows agent's peak capability\n",
    " - **Convergence Speed:** Episodes needed to reach stable performance\n",
    " - **Stability:** Variance in rewards (lower variance = more consistent)\n",
    " \n",
    "**Why Traditional ML Metrics Don't Apply:**\n",
    "\n",
    "In supervised learning, we evaluate models using accuracy, precision, recall, and F1 scores because there are clear target outputs for each input. RL is fundamentally different - the agent learns a policy to maximize cumulative reward through environment interaction, not classification.\n",
    "\n",
    "- **No \"correct labels\"**: There's no ground truth action for each state - only rewards that indicate how good actions were\n",
    "- **Cumulative reward vs per-sample accuracy**: Success is measured over entire episodes, not individual predictions\n",
    "- **Overfitting in RL**: Manifests as \"primacy bias\" where the agent overfits to early experiences and fails to adapt to new situations. Experience replay helps mitigate this.\n",
    "- **Underfitting in RL**: Occurs when the network is too simple or preprocessing removes important visual information, leading to suboptimal policies\n",
    "\n",
    "**Metrics we use instead:**\n",
    "- Average episode reward (higher = better)\n",
    "- Reward variance/stability (lower = more consistent)\n",
    "- Convergence speed (episodes to reach threshold)\n",
    "- Comparison against known benchmarks (random, human, published DQN results)\n",
    " \n",
    " **Space Invaders Benchmarks:**\n",
    " - Random agent: ~150 points\n",
    " - Human performance: ~1,652 points\n",
    " - DQN (Nature paper): ~1,976 points (121% human)\n",
    " - Our target: 800-1500 (Standard DQN), 1000-2000 (Double DQN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41dcc870",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(agent, n_episodes=30, verbose=True):\n",
    "    \"\"\"Evaluate trained agent over multiple episodes.\"\"\"\n",
    "    \n",
    "    env = gym.make('ALE/SpaceInvaders-v5', frameskip=3)\n",
    "    env = SpaceInvadersPreprocessor(env)\n",
    "    \n",
    "    rewards = []\n",
    "    episode_lengths = []\n",
    "    \n",
    "    for ep in range(n_episodes):\n",
    "        state, _ = env.reset()\n",
    "        episode_reward = 0\n",
    "        steps = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            action = agent.choose_action(state, training=False)\n",
    "            state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            episode_reward += reward\n",
    "            steps += 1\n",
    "        \n",
    "        rewards.append(episode_reward)\n",
    "        episode_lengths.append(steps)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"  Episode {ep+1}/{n_episodes}: Reward = {episode_reward}\")\n",
    "    \n",
    "    env.close()\n",
    "    \n",
    "    stats = {\n",
    "        'mean_reward': np.mean(rewards),\n",
    "        'std_reward': np.std(rewards),\n",
    "        'max_reward': np.max(rewards),\n",
    "        'min_reward': np.min(rewards),\n",
    "        'mean_length': np.mean(episode_lengths),\n",
    "        'all_rewards': rewards\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nEvaluation Results ({n_episodes} episodes):\")\n",
    "    print(f\"  Mean Reward:  {stats['mean_reward']:.1f} ± {stats['std_reward']:.1f}\")\n",
    "    print(f\"  Max Reward:   {stats['max_reward']:.0f}\")\n",
    "    print(f\"  Min Reward:   {stats['min_reward']:.0f}\")\n",
    "    print(f\"  Mean Length:  {stats['mean_length']:.0f} steps\")\n",
    "    \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "independent_research_md",
   "metadata": {},
   "source": [
    "## 7. Independent Research: Double DQN and Maximization Bias\n",
    "\n",
    "### The Problem: Maximization Bias in Standard DQN\n",
    "\n",
    "In standard DQN, we use the same network to both **select** the best action and **evaluate** its value:\n",
    "\n",
    "```\n",
    "target = r + γ * max_a Q_target(s', a)\n",
    "```\n",
    "\n",
    "This creates systematic overestimation because:\n",
    "1. The max operator introduces positive bias - when Q-values have estimation noise, taking the maximum preferentially selects overestimated values\n",
    "2. Overestimated actions get selected and reinforced, leading to suboptimal policies\n",
    "3. The error compounds over training as overestimated Q-values propagate through the Bellman backup\n",
    "\n",
    "### The Solution: Double DQN\n",
    "\n",
    "Double DQN (van Hasselt et al., 2016) addresses this by decoupling action selection from action evaluation:\n",
    "\n",
    "```\n",
    "# Standard DQN (overestimates):\n",
    "target = r + γ * Q_target(s', argmax_a Q_target(s', a))\n",
    "\n",
    "# Double DQN (more accurate):\n",
    "best_action = argmax_a Q_online(s', a)    # Online network SELECTS\n",
    "target = r + γ * Q_target(s', best_action) # Target network EVALUATES\n",
    "```\n",
    "\n",
    "The key insight is using **two different value estimates**:\n",
    "- The **online network** selects which action is best\n",
    "- The **target network** evaluates how good that action actually is\n",
    "\n",
    "This breaks the correlation between selection and evaluation, reducing overestimation.\n",
    "\n",
    "### Implementation in Our Code\n",
    "\n",
    "The Double DQN modification is implemented in the `train_step()` method of our DQNAgent class:\n",
    "\n",
    "```python\n",
    "with torch.no_grad():\n",
    "    if self.double_dqn:\n",
    "        # Double DQN: online network selects, target network evaluates\n",
    "        next_actions = self.online_net(next_states).argmax(1, keepdim=True)\n",
    "        next_q = self.target_net(next_states).gather(1, next_actions).squeeze(1)\n",
    "    else:\n",
    "        # Standard DQN: target network does both\n",
    "        next_q = self.target_net(next_states).max(1)[0]\n",
    "    \n",
    "    target_q = rewards + self.gamma * next_q * (1 - dones)\n",
    "```\n",
    "\n",
    "### Why Double DQN Works for Space Invaders\n",
    "\n",
    "Space Invaders has many situations where multiple actions have similar values (e.g., slight left vs slight right when no immediate threat). Standard DQN randomly overestimates one position, potentially learning unstable policies. Double DQN provides more consistent value estimates, leading to:\n",
    "- More stable training curves\n",
    "- Higher final scores\n",
    "- More consistent gameplay behavior\n",
    "\n",
    "\n",
    "### Other Techniques to Improve DQN Performance\n",
    "\n",
    "**Catastrophic Forgetting**\n",
    "\n",
    "Catastrophic forgetting occurs when a neural network abruptly forgets previously learned information upon learning new data. In DQN, this is addressed through the experience replay buffer - by storing past transitions and sampling randomly from them, the agent learns from a diverse mix of old and new experiences rather than just recent ones. Our implementation uses a replay buffer of 100,000 transitions for this purpose.\n",
    "\n",
    "**Random Seed Initialization**\n",
    "\n",
    "Setting random seeds affects reproducibility and exploration behavior. The same seed produces identical random sequences, useful for debugging and comparing algorithms. However, relying on a single seed risks over-optimizing for that specific sequence, so robust evaluation typically averages results across multiple seeds.\n",
    "\n",
    "**Regularization Techniques**\n",
    "\n",
    "While we use gradient clipping (max_norm=10) for training stability, other regularization approaches like L2 weight decay, dropout, or batch normalization could potentially improve generalization. However, the original DQN paper achieved strong results without these, so we followed that approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training_header_md",
   "metadata": {},
   "source": [
    "## 8. Training and Results\n",
    "\n",
    "We train both Standard DQN and Double DQN agents to compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train_standard_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Standard DQN\n",
    "\n",
    "agent_standard, history_standard = train(\n",
    "    episodes=2000, \n",
    "    log_interval=10,\n",
    "    double_dqn=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot_standard_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Standard DQN training curves\n",
    "plot_training_curves(history_standard, title_prefix='Standard DQN', \n",
    "                    save_path='outputs/standard_dqn_training.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca57de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_variance(history_standard, save_path='outputs/variance_standard.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91e6fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rewards_vs_frames(history_standard, title_prefix='Standard DQN',\n",
    "                      save_path='outputs/standard_dqn_frames.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train_double_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Double DQN\n",
    "\n",
    "agent_double, history_double = train(\n",
    "    episodes=2000,  \n",
    "    log_interval=10,\n",
    "    double_dqn=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot_double_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Double DQN training curves\n",
    "plot_training_curves(history_double, title_prefix='Double DQN',\n",
    "                    save_path='outputs/double_dqn_training.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4769e62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_variance(history_double, save_path='outputs/variance_double.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d583c656",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rewards_vs_frames(history_double, title_prefix='Double DQN',\n",
    "                      save_path='outputs/double_dqn_frames.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparison_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Standard vs Double DQN\n",
    "plot_comparison(history_standard, history_double,\n",
    "               save_path='outputs/dqn_comparison.png')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "for history, label, color in [(history_standard, 'Standard DQN', 'blue'),\n",
    "                               (history_double, 'Double DQN', 'red')]:\n",
    "    frames = history['total_frames']\n",
    "    rewards = history['episode_rewards']\n",
    "    if len(rewards) >= 100:\n",
    "        ma = np.convolve(rewards, np.ones(100)/100, mode='valid')\n",
    "        ax.plot(frames[99:], ma, label=label, color=color, linewidth=2)\n",
    "\n",
    "ax.set_xlabel('Total Frames')\n",
    "ax.set_ylabel('Average Reward (100-ep MA)')\n",
    "ax.set_title('Comparison: Rewards vs Frames')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.savefig('outputs/comparison_frames.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\nSummary:\")\n",
    "print(f\"Standard DQN - Final 100-ep avg: {np.mean(history_standard['episode_rewards'][-100:]):.1f}\")\n",
    "print(f\"Double DQN   - Final 100-ep avg: {np.mean(history_double['episode_rewards'][-100:]):.1f}\")\n",
    "print(f\"Standard DQN - Max score: {max(history_standard['episode_rewards']):.0f}\")\n",
    "print(f\"Double DQN   - Max score: {max(history_double['episode_rewards']):.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eval_agents_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate both agents\n",
    "print(\"\\nEvaluating Standard DQN:\")\n",
    "stats_standard = evaluate_agent(agent_standard, n_episodes=30)\n",
    "\n",
    "print(\"\\nEvaluating Double DQN:\")\n",
    "stats_double = evaluate_agent(agent_double, n_episodes=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "record_videos_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record videos for demonstration\n",
    "\n",
    "# Random agent baseline\n",
    "print(\"Recording random agent\")\n",
    "record_random_agent('outputs/random_agent.mp4')\n",
    "\n",
    "# Trained agents\n",
    "print(\"\\nRecording Standard DQN agent\")\n",
    "record_video(agent_standard, 'outputs/standard_dqn_agent.mp4')\n",
    "\n",
    "print(\"\\nRecording Double DQN agent\")\n",
    "record_video(agent_double, 'outputs/double_dqn_agent.mp4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "results_analysis_md",
   "metadata": {},
   "source": [
    "### Analysis of Results\n",
    "\n",
    "**Expected Observations:**\n",
    "\n",
    "1. **Q-Value Estimates:** Double DQN should show lower Q-value estimates than Standard DQN because it reduces overestimation bias. Higher Q-values in Standard DQN don't mean better performance - they often indicate inflated estimates.\n",
    "\n",
    "2. **Training Stability:** Double DQN typically shows more stable training curves with lower variance in episode rewards.\n",
    "\n",
    "3. **Final Performance:** Double DQN often achieves higher final scores, particularly in games like Space Invaders where many actions have similar values.\n",
    "\n",
    "4. **Video Analysis:** The trained agents should show strategic behavior like:\n",
    "   - Positioning to avoid alien fire\n",
    "   - Targeting specific aliens (higher rows for more points)\n",
    "   - Using shields for cover\n",
    "   - Attempting to hit the mystery ship when it appears"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "references_md",
   "metadata": {},
   "source": [
    "## 9. References\n",
    "\n",
    "1. Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., ... & Hassabis, D. (2015). Human-level control through deep reinforcement learning. *Nature*, 518(7540), 529-533.\n",
    "\n",
    "2. van Hasselt, H., Guez, A., & Silver, D. (2016). Deep reinforcement learning with double Q-learning. *Proceedings of the AAAI Conference on Artificial Intelligence*, 30(1).\n",
    "\n",
    "3. Sutton, R. S., & Barto, A. G. (2018). *Reinforcement Learning: An Introduction* (2nd ed.). MIT Press.\n",
    "\n",
    "4. Gymnasium Documentation. https://gymnasium.farama.org/\n",
    "\n",
    "5. CS4287 Lecture M: Deep Q-Networks for Atari Games."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
